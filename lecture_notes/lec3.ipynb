{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecutre 3\n",
    "\n",
    "## Recall:\n",
    "TODO:\n",
    "\n",
    "1. Define a loss function that quantifies our unhappiness with the scors across the training data\n",
    "2. Come up with a way of efficiently finding the parameters that minimize the loss function (optimization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "A loss function tells how good our current classifier is.\n",
    "\n",
    "Given a dataset of examples\n",
    "\n",
    "$$\n",
    "\\{(x_i, y_i)\\}^N _{i=1}\n",
    "$$\n",
    "\n",
    "where $x_i$ is an image and $y_i$ is an integer label.\n",
    "\n",
    "Loss over the dataset is a sum of loss over examples:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum _i L_i(f(x_i, W), y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class SVM Loss\n",
    "Let s be the predicted score coming out of the classifier $s = f(x_i, W)$\n",
    "\n",
    "For example, if our classes were 1 for cat and 2 for dog, then $s_1$ and $s_2$ would be cat and dog scores respectively.\n",
    "\n",
    "$y_i$ was the category of the ground truth label, which is some integer.\n",
    "\n",
    "SO then $s_{y_i}$ would correspond to the score of the **true class** for the i'th example in the training set.\n",
    "\n",
    "The SVM has the form:\n",
    "\n",
    "if $$s_{y_i} \\geq s_j + 1$$\n",
    "then \n",
    "$$\n",
    "L_i = \\sum_{j \\neq y_i} = 0\n",
    "$$\n",
    "otherwise\n",
    "$$\n",
    "L_i = \\sum_{j\\neq y_i} s_j - s_{y_i} + 1\n",
    "$$\n",
    "\n",
    "In summary:\n",
    "\n",
    "$$\n",
    "L_i = \\sum_{j\\neq y_i} max(0, s_j - s_{y_i} + 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: What's the min/max possible loss for SVM?\n",
    "\n",
    "A: Our min loss is 0, if across all our classes our loss was 0.\n",
    "Our max loss is infinity. Recall the hinge joint function \n",
    "\n",
    "Q2: At initialization W is small so all $s \\approx 0$. What is the loss?\n",
    "\n",
    "A: About $c-1$, where c is the number of classes. This is because for each class, if we loop over all incorrect classes, each pairing will have an $s$ that are about the same, so we'll get a loss of 1 as defined in our function (the safety margin).\n",
    "\n",
    "So, we'll get $c-1$, 1's.\n",
    "\n",
    "This is also a useful debugging tool, that you can use as a sanity check at the beginning of training to make sure your code isn't broken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: What is the sum was over all classes, including $j = y_i$?\n",
    "\n",
    "A: The loss increases by 1. (So now $C$ instead of $C-1$)\n",
    "\n",
    "Q4: What if we used mean instead of sum here?\n",
    "$L_i = \\sum_{j\\neq y_i} max(0, s_j - s_{y_i} + 1)$\n",
    "\n",
    "A: Answer wouldn't change. The number of classes is fixed ahead of time when picking the dataset. Remember, we don't actually care about the true values of the scores/loss, only that the scores are higher for the accurate label.\n",
    "\n",
    "Q5: What if we used squared loss?\n",
    "$L_i = \\sum_{j\\neq y_i} max(0, s_j - s_{y_i} + 1) ^2$\n",
    "\n",
    "A: This would be different. We trade good/badness in a nonlinear way, so it would compute a different loss function.\n",
    "\n",
    "Looks like a squared hinge loss.\n",
    "\n",
    "Q6: When squared over not squared?\n",
    "\n",
    "A: Things that are very bad will now be very very bad. So use squared for cases where you don't want huge misclassifications. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
