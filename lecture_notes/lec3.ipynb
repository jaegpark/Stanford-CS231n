{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecutre 3\n",
    "\n",
    "## Recall:\n",
    "TODO:\n",
    "\n",
    "1. Define a loss function that quantifies our unhappiness with the scors across the training data\n",
    "2. Come up with a way of efficiently finding the parameters that minimize the loss function (optimization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "A loss function tells how good our current classifier is.\n",
    "\n",
    "Given a dataset of examples\n",
    "\n",
    "$$\n",
    "\\{(x_i, y_i)\\}^N _{i=1}\n",
    "$$\n",
    "\n",
    "where $x_i$ is an image and $y_i$ is an integer label.\n",
    "\n",
    "Loss over the dataset is a sum of loss over examples:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum _i L_i(f(x_i, W), y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class SVM Loss\n",
    "Let s be the predicted score coming out of the classifier $s = f(x_i, W)$\n",
    "\n",
    "For example, if our classes were 1 for cat and 2 for dog, then $s_1$ and $s_2$ would be cat and dog scores respectively.\n",
    "\n",
    "$y_i$ was the category of the ground truth label, which is some integer.\n",
    "\n",
    "SO then $s_{y_i}$ would correspond to the score of the **true class** for the i'th example in the training set.\n",
    "\n",
    "The SVM has the form:\n",
    "\n",
    "if $$s_{y_i} \\geq s_j + 1$$\n",
    "then \n",
    "$$\n",
    "L_i = \\sum_{j \\neq y_i} = 0\n",
    "$$\n",
    "otherwise\n",
    "$$\n",
    "L_i = \\sum_{j\\neq y_i} s_j - s_{y_i} + 1\n",
    "$$\n",
    "\n",
    "In summary:\n",
    "\n",
    "$$\n",
    "L_i = \\sum_{j\\neq y_i} max(0, s_j - s_{y_i} + 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: What's the min/max possible loss for SVM?\n",
    "\n",
    "A: Our min loss is 0, if across all our classes our loss was 0.\n",
    "Our max loss is infinity. Recall the hinge joint function \n",
    "\n",
    "Q2: At initialization W is small so all $s \\approx 0$. What is the loss?\n",
    "\n",
    "A: About $c-1$, where c is the number of classes. This is because for each class, if we loop over all incorrect classes, each pairing will have an $s$ that are about the same, so we'll get a loss of 1 as defined in our function (the safety margin).\n",
    "\n",
    "So, we'll get $c-1$, 1's.\n",
    "\n",
    "This is also a useful debugging tool, that you can use as a sanity check at the beginning of training to make sure your code isn't broken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: What is the sum was over all classes, including $j = y_i$?\n",
    "\n",
    "A: The loss increases by 1. (So now $C$ instead of $C-1$)\n",
    "\n",
    "Q4: What if we used mean instead of sum here?\n",
    "$L_i = \\sum_{j\\neq y_i} max(0, s_j - s_{y_i} + 1)$\n",
    "\n",
    "A: Answer wouldn't change. The number of classes is fixed ahead of time when picking the dataset. Remember, we don't actually care about the true values of the scores/loss, only that the scores are higher for the accurate label.\n",
    "\n",
    "Q5: What if we used squared loss?\n",
    "$L_i = \\sum_{j\\neq y_i} max(0, s_j - s_{y_i} + 1) ^2$\n",
    "\n",
    "A: This would be different. We trade good/badness in a nonlinear way, so it would compute a different loss function.\n",
    "\n",
    "Looks like a squared hinge loss.\n",
    "\n",
    "Q6: When squared over not squared?\n",
    "\n",
    "A: Things that are very bad will now be very very bad. So use squared for cases where you don't want huge misclassifications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass SVM (Support Vector Machine) Loss\n",
    "Example code:\n",
    "\n",
    "$$\n",
    "L_i = \\sum_{j\\neq y_i} max(0, s_j - s_{y_i} + 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def L_i_vectorized(x, y, W):\n",
    "    scores = W.dot(x)\n",
    "    margins = np.maximum(0, scores - scores[y] + 1)\n",
    "    margins[y] = 0\n",
    "    loss_i = np.sum(margins)\n",
    "    return loss_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppose that we found a W such that L = 0. Is this W unique?\n",
    "Recall:\n",
    "\n",
    "$$\n",
    "f(x, W) = Wx\n",
    "$$\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum^N_{i=1} \\sum_{j\\neq y_i} max(0, s_j - s_{y_i} + 1)\n",
    "$$\n",
    "\n",
    "Answer: **NO!** Not unique. For example, $2W$ is also now $L=0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "Data loss: model predictions should match training data\n",
    "\n",
    "Regularization: Model should be \"simple\", so it works on test data. \n",
    "\n",
    "AKA, we don't want to overfit our training data.\n",
    "\n",
    "So, we introduce a new term in our loss function:\n",
    "\n",
    "$$\n",
    "L(W) = \\frac{1}{N} \\sum^N_{i=1} L_i(f(x_i, W), y_i) + \\lambda R(W)\n",
    "$$\n",
    "\n",
    "#### Occam's Razor:\n",
    "\"Among competing hypotheses, the simplest is best\"\n",
    "William of Ockahm, 1285-1347."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Types\n",
    "\n",
    "$\\lambda = $ regularization strength (**hyperparameter**)\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum^N_{i=1} \\sum_{j\\neq y_i} max(0, f(x_i; W)_j - f(x_i;W)_{y_i} + 1) + \\lambda R(W)\n",
    "$$\n",
    "\n",
    "#### In common use:\n",
    "**L2 regularization** $R(W) = \\sum_k \\sum_l W^2 _{k,l}$\n",
    "\n",
    "L1 regularization $R(W) = \\sum_k \\sum_l |W _{k,l}|$\n",
    "\n",
    "Elastic net (L1 + L2) $R(W) = \\sum_k \\sum_l \\beta W^2 _{k,l} + |W _{k,l}|$\n",
    "\n",
    "Max norm regularization (might see later)\n",
    "\n",
    "Dropout (will see later)\n",
    "\n",
    "Fancier: Batch normalization, stocastic depth\n",
    "\n",
    "---\n",
    "A nice way of thinking about L1 vs L2 is:\n",
    "\n",
    "L1 prefers spare solutions, that drives all your entries of W to 0 except for a couple. L2 prefers to spread the W across all the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classifier\n",
    "AKA: Multinominal Logistic Regression\n",
    "\n",
    "scores = unnormalized log probabilities of the classes.\n",
    "$$\n",
    "P(Y=k|X=x_i) = \\frac{e^{s_k} }{\\sum_j e^{s_j}}\n",
    "$$\n",
    "where $s = f(x_i;W)$\n",
    "\n",
    "We want to maximize the log liklihood, or (for a loss function), to minimize the negative log likelihood of the correct class:\n",
    "\n",
    "$$\n",
    "L_i = -\\log P(Y=y_i | X= x_i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Sanity check: if all the s's are small and about 0, what is the loss?\n",
    "\n",
    "A: $-\\log(\\frac{1}{C}) = \\log(C)$\n",
    "\n",
    "\n",
    "An interesting difference between softmax and SVM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM will not care once a point is correctly classified. \n",
    "\n",
    "Softmax will always want you to push the score of the correct class to infiity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "- We have some dataset of (x,y)\n",
    "- We have a score function\n",
    "- We have a loss function (many choices: Softmax, SVM, Full loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "E.g. mountain:\n",
    "\n",
    "Slope in any direction is dot product of direction with gradient.\n",
    "Direction of steepest descent is negative gradient.\n",
    "\n",
    "Summary:\n",
    "- Numerical gradient: approximate, slow, easy to write\n",
    "- Analytic gradient: exact, fast, error-prone\n",
    "\n",
    "In practice, always use analytic gradient, but check implementation with a numerical gradient. This is called a gradient check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanila gradient descent\n",
    "while True:\n",
    "    weights_grad = evaluate_gradient(loss_fun, data, weights)\n",
    "    weights += - step_size * weights_grad # performs parameter update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step_size is an important hyperparameter - this is commonly known as the **learning rate**.\n",
    "\n",
    "There are also different update rules which tell us how exactly to use the gradient information at every time step (e.g. Adam, SGD, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)\n",
    "Full sum for our loss function over every single image is slow and costly.\n",
    "\n",
    "What we can instead do is approximate using a **minibatch** of examples (32, 64, 128 are common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla Minibatch Gradient Descent\n",
    "\n",
    "while True:\n",
    "    data_batch = sample_training_data(data, 256)\n",
    "    weights_grad = evaluate_gradient(loss_fun, data_batch, weights)\n",
    "    weights += - step_size * weights_grad"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "205a18728344f39db965b2d9cebb2d1ea3cf944444e8838c73d6b0832603cfd3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
